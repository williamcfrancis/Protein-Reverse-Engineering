{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ees3CdiFWPqC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRzAPXZ8XK0g",
        "outputId": "d617c738-3a89-496a-b6c2-2989bcf8d9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 1160\n",
        "AMINO_VOCAB_SIZE = 22     # 20 amino acids + <PAD> + <SOS>\n",
        "CODON_VOCAB_SIZE = 67     # 64 codons + <PAD> + <SOS> + <EOS>\n",
        "EMBED_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 4\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 300\n",
        "PATIENCE = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "JBSQZxryXcpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.load(\"/content/drive/MyDrive/X_train_t.npy\")\n",
        "Y_train = np.load(\"/content/drive/MyDrive/Y_train_t.npy\")\n",
        "X_test  = np.load(\"/content/drive/MyDrive/X_test_t.npy\")\n",
        "Y_test  = np.load(\"/content/drive/MyDrive/Y_test_t.npy\")"
      ],
      "metadata": {
        "id": "KjxKJOSLXgmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(Y_train))\n",
        "test_data = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(Y_test))\n"
      ],
      "metadata": {
        "id": "Z0Dpt6gpXlvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "wXvfbZATXnli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinToDNATransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder_embed = nn.Embedding(AMINO_VOCAB_SIZE, EMBED_DIM)\n",
        "        self.decoder_embed = nn.Embedding(CODON_VOCAB_SIZE, EMBED_DIM)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=EMBED_DIM,\n",
        "            nhead=NUM_HEADS,\n",
        "            num_encoder_layers=NUM_LAYERS,\n",
        "            num_decoder_layers=NUM_LAYERS,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(128, CODON_VOCAB_SIZE)\n",
        "\n",
        "    def generate_padding_mask(self, seq, pad_token):\n",
        "        return (seq == pad_token)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.generate_padding_mask(src, pad_token=21)  # amino <PAD>\n",
        "        tgt_mask = self.generate_padding_mask(tgt, pad_token=65)  # codon <PAD>\n",
        "\n",
        "        src_embed = self.encoder_embed(src)\n",
        "        tgt_embed = self.decoder_embed(tgt)\n",
        "\n",
        "        out = self.transformer(\n",
        "            src_embed,\n",
        "            tgt_embed,\n",
        "            src_key_padding_mask=src_mask,\n",
        "            tgt_key_padding_mask=tgt_mask,\n",
        "            memory_key_padding_mask=src_mask\n",
        "        )\n",
        "        return self.fc_out(out)"
      ],
      "metadata": {
        "id": "ViH6nxatXrRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ProteinToDNATransformer().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=65)  # 65 = <PAD>\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "5DevaBCSXrpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/content/drive/MyDrive/models\", exist_ok=True)\n",
        "losses = []\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0"
      ],
      "metadata": {
        "id": "W6vFh5hmXwxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]  # input starts with <SOS>\n",
        "        tgt_output = tgt[:, 1:]  # output ends with <EOS>\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        loss = criterion(output.view(-1, CODON_VOCAB_SIZE), tgt_output.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/models/best_transformer_with_sos.pt\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "F_oF3cv3Xz7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9332432e-5f23-404a-c04a-2f40a9f3108e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 - Loss: 2.0136\n",
            "Epoch 2/300 - Loss: 1.3622\n",
            "Epoch 3/300 - Loss: 1.2088\n",
            "Epoch 4/300 - Loss: 1.1822\n",
            "Epoch 5/300 - Loss: 1.1706\n",
            "Epoch 6/300 - Loss: 1.1634\n",
            "Epoch 7/300 - Loss: 1.1580\n",
            "Epoch 8/300 - Loss: 1.1532\n",
            "Epoch 9/300 - Loss: 1.1479\n",
            "Epoch 10/300 - Loss: 1.1440\n",
            "Epoch 11/300 - Loss: 1.1409\n",
            "Epoch 12/300 - Loss: 1.1387\n",
            "Epoch 13/300 - Loss: 1.1369\n",
            "Epoch 14/300 - Loss: 1.1350\n",
            "Epoch 15/300 - Loss: 1.1332\n",
            "Epoch 16/300 - Loss: 1.1315\n",
            "Epoch 17/300 - Loss: 1.1301\n",
            "Epoch 18/300 - Loss: 1.1293\n",
            "Epoch 19/300 - Loss: 1.1278\n",
            "Epoch 20/300 - Loss: 1.1267\n",
            "Epoch 21/300 - Loss: 1.1258\n",
            "Epoch 22/300 - Loss: 1.1252\n",
            "Epoch 23/300 - Loss: 1.1243\n",
            "Epoch 24/300 - Loss: 1.1235\n",
            "Epoch 25/300 - Loss: 1.1230\n",
            "Epoch 26/300 - Loss: 1.1224\n",
            "Epoch 27/300 - Loss: 1.1217\n",
            "Epoch 28/300 - Loss: 1.1213\n",
            "Epoch 29/300 - Loss: 1.1207\n",
            "Epoch 30/300 - Loss: 1.1200\n",
            "Epoch 31/300 - Loss: 1.1196\n",
            "Epoch 32/300 - Loss: 1.1191\n",
            "Epoch 33/300 - Loss: 1.1189\n",
            "Epoch 34/300 - Loss: 1.1182\n",
            "Epoch 35/300 - Loss: 1.1175\n",
            "Epoch 36/300 - Loss: 1.1172\n",
            "Epoch 37/300 - Loss: 1.1168\n",
            "Epoch 38/300 - Loss: 1.1163\n",
            "Epoch 39/300 - Loss: 1.1159\n",
            "Epoch 40/300 - Loss: 1.1153\n",
            "Epoch 41/300 - Loss: 1.1146\n",
            "Epoch 42/300 - Loss: 1.1141\n",
            "Epoch 43/300 - Loss: 1.1135\n",
            "Epoch 44/300 - Loss: 1.1128\n",
            "Epoch 45/300 - Loss: 1.1123\n",
            "Epoch 46/300 - Loss: 1.1115\n",
            "Epoch 47/300 - Loss: 1.1105\n",
            "Epoch 48/300 - Loss: 1.1096\n",
            "Epoch 49/300 - Loss: 1.1093\n",
            "Epoch 50/300 - Loss: 1.1084\n",
            "Epoch 51/300 - Loss: 1.1079\n",
            "Epoch 52/300 - Loss: 1.1070\n",
            "Epoch 53/300 - Loss: 1.1061\n",
            "Epoch 54/300 - Loss: 1.1065\n",
            "Epoch 55/300 - Loss: 1.1055\n",
            "Epoch 56/300 - Loss: 1.1051\n",
            "Epoch 57/300 - Loss: 1.1042\n",
            "Epoch 58/300 - Loss: 1.1041\n",
            "Epoch 59/300 - Loss: 1.1035\n",
            "Epoch 60/300 - Loss: 1.1028\n",
            "Epoch 61/300 - Loss: 1.1026\n",
            "Epoch 62/300 - Loss: 1.1020\n",
            "Epoch 63/300 - Loss: 1.1014\n",
            "Epoch 64/300 - Loss: 1.1011\n",
            "Epoch 65/300 - Loss: 1.1006\n",
            "Epoch 66/300 - Loss: 1.0999\n",
            "Epoch 67/300 - Loss: 1.0997\n",
            "Epoch 68/300 - Loss: 1.0991\n",
            "Epoch 69/300 - Loss: 1.0985\n",
            "Epoch 70/300 - Loss: 1.0984\n",
            "Epoch 71/300 - Loss: 1.0977\n",
            "Epoch 72/300 - Loss: 1.0973\n",
            "Epoch 73/300 - Loss: 1.0968\n",
            "Epoch 74/300 - Loss: 1.0963\n",
            "Epoch 75/300 - Loss: 1.0958\n",
            "Epoch 76/300 - Loss: 1.0954\n",
            "Epoch 77/300 - Loss: 1.0948\n",
            "Epoch 78/300 - Loss: 1.0947\n",
            "Epoch 79/300 - Loss: 1.0943\n",
            "Epoch 80/300 - Loss: 1.0940\n",
            "Epoch 81/300 - Loss: 1.0932\n",
            "Epoch 82/300 - Loss: 1.0929\n",
            "Epoch 83/300 - Loss: 1.0922\n",
            "Epoch 84/300 - Loss: 1.0918\n",
            "Epoch 85/300 - Loss: 1.0915\n",
            "Epoch 86/300 - Loss: 1.0913\n",
            "Epoch 87/300 - Loss: 1.0904\n",
            "Epoch 88/300 - Loss: 1.0898\n",
            "Epoch 89/300 - Loss: 1.0896\n",
            "Epoch 90/300 - Loss: 1.0891\n",
            "Epoch 91/300 - Loss: 1.0885\n",
            "Epoch 92/300 - Loss: 1.0881\n",
            "Epoch 93/300 - Loss: 1.0875\n",
            "Epoch 94/300 - Loss: 1.0872\n",
            "Epoch 95/300 - Loss: 1.0871\n",
            "Epoch 96/300 - Loss: 1.0870\n",
            "Epoch 97/300 - Loss: 1.0864\n",
            "Epoch 98/300 - Loss: 1.0858\n",
            "Epoch 99/300 - Loss: 1.0850\n",
            "Epoch 100/300 - Loss: 1.0845\n",
            "Epoch 101/300 - Loss: 1.0839\n",
            "Epoch 102/300 - Loss: 1.0839\n",
            "Epoch 103/300 - Loss: 1.0832\n",
            "Epoch 104/300 - Loss: 1.0833\n",
            "Epoch 105/300 - Loss: 1.0824\n",
            "Epoch 106/300 - Loss: 1.0818\n",
            "Epoch 107/300 - Loss: 1.0814\n",
            "Epoch 108/300 - Loss: 1.0806\n",
            "Epoch 109/300 - Loss: 1.0803\n",
            "Epoch 110/300 - Loss: 1.0797\n",
            "Epoch 111/300 - Loss: 1.0792\n",
            "Epoch 112/300 - Loss: 1.0787\n",
            "Epoch 113/300 - Loss: 1.0782\n",
            "Epoch 114/300 - Loss: 1.0772\n",
            "Epoch 115/300 - Loss: 1.0768\n",
            "Epoch 116/300 - Loss: 1.0766\n",
            "Epoch 117/300 - Loss: 1.0761\n",
            "Epoch 118/300 - Loss: 1.0754\n",
            "Epoch 119/300 - Loss: 1.0750\n",
            "Epoch 120/300 - Loss: 1.0746\n",
            "Epoch 121/300 - Loss: 1.0738\n",
            "Epoch 122/300 - Loss: 1.0738\n",
            "Epoch 123/300 - Loss: 1.0729\n",
            "Epoch 124/300 - Loss: 1.0726\n",
            "Epoch 125/300 - Loss: 1.0722\n",
            "Epoch 126/300 - Loss: 1.0707\n",
            "Epoch 127/300 - Loss: 1.0702\n",
            "Epoch 128/300 - Loss: 1.0689\n",
            "Epoch 129/300 - Loss: 1.0684\n",
            "Epoch 130/300 - Loss: 1.0683\n",
            "Epoch 131/300 - Loss: 1.0676\n",
            "Epoch 132/300 - Loss: 1.0671\n",
            "Epoch 133/300 - Loss: 1.0666\n",
            "Epoch 134/300 - Loss: 1.0656\n",
            "Epoch 135/300 - Loss: 1.0649\n",
            "Epoch 136/300 - Loss: 1.0640\n",
            "Epoch 137/300 - Loss: 1.0637\n",
            "Epoch 138/300 - Loss: 1.0631\n",
            "Epoch 139/300 - Loss: 1.0622\n",
            "Epoch 140/300 - Loss: 1.0620\n",
            "Epoch 141/300 - Loss: 1.0606\n",
            "Epoch 142/300 - Loss: 1.0611\n",
            "Epoch 143/300 - Loss: 1.0591\n",
            "Epoch 144/300 - Loss: 1.0586\n",
            "Epoch 145/300 - Loss: 1.0571\n",
            "Epoch 146/300 - Loss: 1.0567\n",
            "Epoch 147/300 - Loss: 1.0556\n",
            "Epoch 148/300 - Loss: 1.0551\n",
            "Epoch 149/300 - Loss: 1.0547\n",
            "Epoch 150/300 - Loss: 1.0539\n",
            "Epoch 151/300 - Loss: 1.0530\n",
            "Epoch 152/300 - Loss: 1.0523\n",
            "Epoch 153/300 - Loss: 1.0509\n",
            "Epoch 154/300 - Loss: 1.0499\n",
            "Epoch 155/300 - Loss: 1.0491\n",
            "Epoch 156/300 - Loss: 1.0480\n",
            "Epoch 157/300 - Loss: 1.0474\n",
            "Epoch 158/300 - Loss: 1.0466\n",
            "Epoch 159/300 - Loss: 1.0465\n",
            "Epoch 160/300 - Loss: 1.0457\n",
            "Epoch 161/300 - Loss: 1.0449\n",
            "Epoch 162/300 - Loss: 1.0423\n",
            "Epoch 163/300 - Loss: 1.0419\n",
            "Epoch 164/300 - Loss: 1.0404\n",
            "Epoch 165/300 - Loss: 1.0403\n",
            "Epoch 166/300 - Loss: 1.0396\n",
            "Epoch 167/300 - Loss: 1.0397\n",
            "Epoch 168/300 - Loss: 1.0386\n",
            "Epoch 169/300 - Loss: 1.0366\n",
            "Epoch 170/300 - Loss: 1.0373\n",
            "Epoch 171/300 - Loss: 1.0363\n",
            "Epoch 172/300 - Loss: 1.0341\n",
            "Epoch 173/300 - Loss: 1.0324\n",
            "Epoch 174/300 - Loss: 1.0314\n",
            "Epoch 175/300 - Loss: 1.0299\n",
            "Epoch 176/300 - Loss: 1.0285\n",
            "Epoch 177/300 - Loss: 1.0279\n",
            "Epoch 178/300 - Loss: 1.0271\n",
            "Epoch 179/300 - Loss: 1.0252\n",
            "Epoch 180/300 - Loss: 1.0249\n",
            "Epoch 181/300 - Loss: 1.0245\n",
            "Epoch 182/300 - Loss: 1.0231\n",
            "Epoch 183/300 - Loss: 1.0216\n",
            "Epoch 184/300 - Loss: 1.0207\n",
            "Epoch 185/300 - Loss: 1.0192\n",
            "Epoch 186/300 - Loss: 1.0181\n",
            "Epoch 187/300 - Loss: 1.0164\n",
            "Epoch 188/300 - Loss: 1.0150\n",
            "Epoch 189/300 - Loss: 1.0141\n",
            "Epoch 190/300 - Loss: 1.0122\n",
            "Epoch 191/300 - Loss: 1.0120\n",
            "Epoch 192/300 - Loss: 1.0109\n",
            "Epoch 193/300 - Loss: 1.0096\n",
            "Epoch 194/300 - Loss: 1.0092\n",
            "Epoch 195/300 - Loss: 1.0077\n",
            "Epoch 196/300 - Loss: 1.0067\n",
            "Epoch 197/300 - Loss: 1.0054\n",
            "Epoch 198/300 - Loss: 1.0039\n",
            "Epoch 199/300 - Loss: 1.0027\n",
            "Epoch 200/300 - Loss: 1.0012\n",
            "Epoch 201/300 - Loss: 1.0006\n",
            "Epoch 202/300 - Loss: 0.9988\n",
            "Epoch 203/300 - Loss: 0.9986\n",
            "Epoch 204/300 - Loss: 0.9960\n",
            "Epoch 205/300 - Loss: 0.9939\n",
            "Epoch 206/300 - Loss: 0.9928\n",
            "Epoch 207/300 - Loss: 0.9924\n",
            "Epoch 208/300 - Loss: 0.9910\n",
            "Epoch 209/300 - Loss: 0.9896\n",
            "Epoch 210/300 - Loss: 0.9886\n",
            "Epoch 211/300 - Loss: 0.9865\n",
            "Epoch 212/300 - Loss: 0.9861\n",
            "Epoch 213/300 - Loss: 0.9851\n",
            "Epoch 214/300 - Loss: 0.9835\n",
            "Epoch 215/300 - Loss: 0.9818\n",
            "Epoch 216/300 - Loss: 0.9807\n",
            "Epoch 217/300 - Loss: 0.9792\n",
            "Epoch 218/300 - Loss: 0.9790\n",
            "Epoch 219/300 - Loss: 0.9780\n",
            "Epoch 220/300 - Loss: 0.9772\n",
            "Epoch 221/300 - Loss: 0.9748\n",
            "Epoch 222/300 - Loss: 0.9735\n",
            "Epoch 223/300 - Loss: 0.9717\n",
            "Epoch 224/300 - Loss: 0.9705\n",
            "Epoch 225/300 - Loss: 0.9694\n",
            "Epoch 226/300 - Loss: 0.9682\n",
            "Epoch 227/300 - Loss: 0.9665\n",
            "Epoch 228/300 - Loss: 0.9653\n",
            "Epoch 229/300 - Loss: 0.9634\n",
            "Epoch 230/300 - Loss: 0.9624\n",
            "Epoch 231/300 - Loss: 0.9605\n",
            "Epoch 232/300 - Loss: 0.9589\n",
            "Epoch 233/300 - Loss: 0.9593\n",
            "Epoch 234/300 - Loss: 0.9576\n",
            "Epoch 235/300 - Loss: 0.9559\n",
            "Epoch 236/300 - Loss: 0.9544\n",
            "Epoch 237/300 - Loss: 0.9525\n",
            "Epoch 238/300 - Loss: 0.9518\n",
            "Epoch 239/300 - Loss: 0.9493\n",
            "Epoch 240/300 - Loss: 0.9486\n",
            "Epoch 241/300 - Loss: 0.9468\n",
            "Epoch 242/300 - Loss: 0.9471\n",
            "Epoch 243/300 - Loss: 0.9461\n",
            "Epoch 244/300 - Loss: 0.9442\n",
            "Epoch 245/300 - Loss: 0.9420\n",
            "Epoch 246/300 - Loss: 0.9402\n",
            "Epoch 247/300 - Loss: 0.9387\n",
            "Epoch 248/300 - Loss: 0.9367\n",
            "Epoch 249/300 - Loss: 0.9358\n",
            "Epoch 250/300 - Loss: 0.9355\n",
            "Epoch 251/300 - Loss: 0.9343\n",
            "Epoch 252/300 - Loss: 0.9326\n",
            "Epoch 253/300 - Loss: 0.9304\n",
            "Epoch 254/300 - Loss: 0.9299\n",
            "Epoch 255/300 - Loss: 0.9290\n",
            "Epoch 256/300 - Loss: 0.9278\n",
            "Epoch 257/300 - Loss: 0.9268\n",
            "Epoch 258/300 - Loss: 0.9235\n",
            "Epoch 259/300 - Loss: 0.9221\n",
            "Epoch 260/300 - Loss: 0.9201\n",
            "Epoch 261/300 - Loss: 0.9182\n",
            "Epoch 262/300 - Loss: 0.9169\n",
            "Epoch 263/300 - Loss: 0.9167\n",
            "Epoch 264/300 - Loss: 0.9160\n",
            "Epoch 265/300 - Loss: 0.9148\n",
            "Epoch 266/300 - Loss: 0.9126\n",
            "Epoch 267/300 - Loss: 0.9126\n",
            "Epoch 268/300 - Loss: 0.9128\n",
            "Epoch 269/300 - Loss: 0.9104\n",
            "Epoch 270/300 - Loss: 0.9086\n",
            "Epoch 271/300 - Loss: 0.9067\n",
            "Epoch 272/300 - Loss: 0.9057\n",
            "Epoch 273/300 - Loss: 0.9055\n",
            "Epoch 274/300 - Loss: 0.9017\n",
            "Epoch 275/300 - Loss: 0.8994\n",
            "Epoch 276/300 - Loss: 0.8985\n",
            "Epoch 277/300 - Loss: 0.8976\n",
            "Epoch 278/300 - Loss: 0.8952\n",
            "Epoch 279/300 - Loss: 0.8953\n",
            "Epoch 280/300 - Loss: 0.8934\n",
            "Epoch 281/300 - Loss: 0.8934\n",
            "Epoch 282/300 - Loss: 0.8942\n",
            "Epoch 283/300 - Loss: 0.8915\n",
            "Epoch 284/300 - Loss: 0.8888\n",
            "Epoch 285/300 - Loss: 0.8881\n",
            "Epoch 286/300 - Loss: 0.8865\n",
            "Epoch 287/300 - Loss: 0.8855\n",
            "Epoch 288/300 - Loss: 0.8848\n",
            "Epoch 289/300 - Loss: 0.8830\n",
            "Epoch 290/300 - Loss: 0.8807\n",
            "Epoch 291/300 - Loss: 0.8790\n",
            "Epoch 292/300 - Loss: 0.8772\n",
            "Epoch 293/300 - Loss: 0.8767\n",
            "Epoch 294/300 - Loss: 0.8747\n",
            "Epoch 295/300 - Loss: 0.8730\n",
            "Epoch 296/300 - Loss: 0.8723\n",
            "Epoch 297/300 - Loss: 0.8701\n",
            "Epoch 298/300 - Loss: 0.8694\n",
            "Epoch 299/300 - Loss: 0.8688\n",
            "Epoch 300/300 - Loss: 0.8662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Qmli6eOX8L9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}